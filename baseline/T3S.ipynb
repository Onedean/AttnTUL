{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import math\n",
    "from math import radians, cos, sin, asin, atan2, sqrt, degrees\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(lon1, lat1, lon2, lat2):\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    c = 2 * asin(sqrt(sin((lat2 - lat1)/2)**2 + cos(lat1) * cos(lat2) * sin((lon2 - lon1)/2)**2))  # haversine公式\n",
    "    r = 6371.393\n",
    "    return c * r * 1000\n",
    "\n",
    "\n",
    "def conut_gird_num(tracks_data, grid_distance):\n",
    "    Lon1 = tracks_data['Lon'].min()\n",
    "    Lat1 = tracks_data['Lat'].min()\n",
    "    Lon2 = tracks_data['Lon'].max()\n",
    "    Lat2 = tracks_data['Lat'].max()\n",
    "    low = haversine_distance(Lon1,Lat1,Lon2,Lat1)\n",
    "    high = haversine_distance(Lon1,Lat2,Lon2,Lat2)\n",
    "    left = haversine_distance(Lon1,Lat1,Lon1,Lat2)\n",
    "    right = haversine_distance(Lon2,Lat1,Lon2,Lat2)\n",
    "    lon_grid_num = int((low + high) / 2 / grid_distance)\n",
    "    lat_grid_num = int((left + right) / 2 /grid_distance)\n",
    "    print(\"before total:\", lon_grid_num, '*', lat_grid_num, '=', lon_grid_num * lat_grid_num, 'grid')\n",
    "    return lon_grid_num, lat_grid_num\n",
    "\n",
    "\n",
    "def grid_process(tracks_data, grid_distance):\n",
    "    lon_grid_num, lat_grid_num = conut_gird_num(tracks_data, grid_distance)\n",
    "    Lon1 = tracks_data['Lon'].min()\n",
    "    Lon2 = tracks_data['Lon'].max()\n",
    "    Lat1 = tracks_data['Lat'].min()\n",
    "    Lat2 = tracks_data['Lat'].max()\n",
    "    Lon_gap = (Lon2 - Lon1)/lon_grid_num\n",
    "    Lat_gap = (Lat2 - Lat1)/lat_grid_num\n",
    "    tracks_data['grid_ID'] = tracks_data.apply(lambda x: int((x['Lat']-Lat1)/Lat_gap) * lon_grid_num + int((x['Lon']-Lon1)/Lon_gap) + 1, axis=1)\n",
    "    sort_grid = sorted(set(tracks_data['grid_ID']))\n",
    "    tracks_data['grid_ID'] = [sort_grid.index(num) for num in tqdm(tracks_data['grid_ID'])]\n",
    "    print('after total:', len(sort_grid), 'grid')\n",
    "    return tracks_data\n",
    "\n",
    "raw_path = '..\\data\\shenzhen\\shenzhen-all.csv'\n",
    "grid_distance = 111\n",
    "\n",
    "tracks_data = pd.read_csv(raw_path, sep='\\t')\n",
    "tracks_data = grid_process(tracks_data, grid_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_gps = tracks_data.apply(lambda x: str(x['Lon'])+'_'+str(x['Lat']), axis=1).drop_duplicates().values.tolist()\n",
    "gps2idx = {gps:idx+1 for idx, gps in enumerate(all_gps)}\n",
    "gps2idx['pad'] = 0\n",
    "\n",
    "\n",
    "all_grid = tracks_data['grid_ID'].drop_duplicates().values.tolist()\n",
    "grid2idx = {grid:idx+1 for idx, grid in enumerate(all_grid)}\n",
    "grid2idx['pad'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test = 0\n",
    "split_ratio  = 0.6\n",
    "valid_size = 0.5\n",
    "\n",
    "\n",
    "user_list = tracks_data['ObjectID'].drop_duplicates().values.tolist()\n",
    "user_traj_dict = {key:[] for key in user_list}\n",
    "for user_id in tqdm(tracks_data['ObjectID'].drop_duplicates().values.tolist()):\n",
    "    one_user_data = tracks_data.loc[tracks_data.ObjectID == user_id, :]\n",
    "    for traj_id in one_user_data['TrajNumber'].drop_duplicates().values.tolist():\n",
    "        single_grid_data = [grid2idx[grid] for grid in one_user_data.loc[tracks_data.TrajNumber == traj_id, 'grid_ID'].values.tolist()]\n",
    "        single_traj_data = [gps2idx[str(x)+'_'+str(y)] for x,y in zip(one_user_data.loc[tracks_data.TrajNumber == traj_id, 'Lon'].values.tolist(), one_user_data.loc[tracks_data.TrajNumber == traj_id, 'Lat'].values.tolist())]\n",
    "        user_traj_dict[user_id].append((single_grid_data, single_traj_data))\n",
    "\n",
    "user_traj_train, user_traj_test = {key:[] for key in user_list}, {key:[] for key in user_list}\n",
    "\n",
    "for key in user_traj_dict:\n",
    "    traj_num = len(user_traj_dict[key])\n",
    "    num_test += traj_num - int(traj_num * split_ratio)\n",
    "    \n",
    "    for idx in list(range(traj_num))[:int(traj_num * split_ratio)]:\n",
    "        user_traj_train[key].append(user_traj_dict[key][idx])\n",
    "    \n",
    "    for idx in list(range(traj_num))[int(traj_num * split_ratio):]:\n",
    "        user_traj_test[key].append(user_traj_dict[key][idx])\n",
    "\n",
    "\n",
    "print('test::', num_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self, train):\n",
    "        alldata = user_traj_train if train else user_traj_test\n",
    "        self.x_seq, self.g_seq, self.label = [], [], []\n",
    "        for key in alldata:\n",
    "            for one_data in alldata[key]:\n",
    "                single_grid_data, single_traj_data = one_data\n",
    "                self.x_seq.append(single_traj_data)\n",
    "                self.g_seq.append(single_grid_data)\n",
    "                self.label.append(key)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_seq[index], self.g_seq[index], self.label[index], len(self.x_seq[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_seq)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = sorted(batch, key=lambda x: x[-1], reverse=True)\n",
    "    x_contents, g_contents, labels, input_lengths = zip(*batch)\n",
    "    max_len = max([len(content) for content in g_contents])\n",
    "    x_contents = torch.LongTensor([content + [0] * (max_len - len(content)) if len(content) < max_len else content for content in x_contents])\n",
    "    g_contents = torch.LongTensor([content + [0] * (max_len - len(content)) if len(content) < max_len else content for content in g_contents])\n",
    "    labels = torch.LongTensor(labels)\n",
    "    input_lengths = torch.LongTensor(input_lengths)\n",
    "    return x_contents, g_contents, labels, input_lengths\n",
    "\n",
    "\n",
    "indices = list(range(num_test))\n",
    "np.random.seed(555)\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(num_test * valid_size))\n",
    "valid_idx, test_idx = indices[split:], indices[:split]\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "test_sampler = SubsetRandomSampler(test_idx)\n",
    "\n",
    "\n",
    "train_dataset = myDataset(train=True)\n",
    "test_dataset = myDataset(train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    }
   ],
   "source": [
    "n_class = len(user_traj_dict)\n",
    "n_hidden = 128\n",
    "grid_embedding_dim = 128\n",
    "print(n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence,pad_packed_sequence\n",
    "\n",
    "class T3S(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(T3S, self).__init__()\n",
    "        # self.emb1 = nn.Embedding(len(gps2idx), 250, padding_idx=0)\n",
    "        self.emb1 = nn.Embedding(len(grid2idx), 250, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=250, hidden_size=n_hidden, batch_first=True, bidirectional=False)\n",
    "        self.emb2 = nn.Embedding(len(grid2idx), 128, padding_idx=0)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=128, nhead=16)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        self.fc = nn.Linear(n_hidden, n_class)\n",
    "\n",
    "    def forward(self, seq_x, seq_g, input_length):\n",
    "        # seq_x = self.emb1(seq_x)\n",
    "        input = self.emb1(seq_g)\n",
    "        x_packed = pack_padded_sequence(input, input_length, batch_first=True)\n",
    "        o_n, (h_n, _) = self.lstm(x_packed)\n",
    "        lstm_output = h_n[-1, :, :]\n",
    "        seq_g = self.emb2(seq_g)\n",
    "        attn_output = self.transformer_encoder(seq_g).mean(dim=1)\n",
    "        \n",
    "        output = self.fc((lstm_output + attn_output))\n",
    "        # output = self.fc(attn_output)\n",
    "        return F.log_softmax(output, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_1(pred, targ):\n",
    "    pred = torch.max(pred, 1)[1]\n",
    "    ac = ((pred == targ).float()).sum().item() / targ.size()[0]\n",
    "    return ac\n",
    "\n",
    "def accuracy_5(pred,targ):\n",
    "    pred = torch.topk(pred, k=5, dim=1, largest=True, sorted=True)[1]\n",
    "    ac = (torch.tensor([t in p for p,t in zip(pred,targ)]).float()).sum().item() / targ.size()[0]\n",
    "    return ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "        \n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), '../temp/checkpoint.pt')\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def train_model(model, patience, n_epochs):\n",
    "    avg_train_losses = []\n",
    "    avg_valid_losses = []\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss_list, y_predict_list, y_true_list, acc1_list, acc5_list = [], [], [], [], []\n",
    "        train_dataloader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True, drop_last=False, collate_fn=collate_fn)\n",
    "        for idx, (seq_x, seq_g, y_true, input_length) in enumerate(train_dataloader):\n",
    "            # seq_x, seq_g, y_true = seq_x.cuda(), seq_g.cuda(), y_true.cuda()\n",
    "            seq_g, y_true = seq_g.cuda(), y_true.cuda()\n",
    "            y_predict = model(seq_x, seq_g, input_length)\n",
    "            y_predict_list.extend(torch.max(y_predict, 1)[1].cpu().numpy().tolist())\n",
    "            y_true_list.extend(y_true.cpu().numpy().tolist())\n",
    "            acc1_list.append(accuracy_1(y_predict, y_true))\n",
    "            acc5_list.append(accuracy_5(y_predict, y_true))\n",
    "            \n",
    "            loss = F.nll_loss(y_predict, y_true)\n",
    "            train_loss_list.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print('Epoch: {}'.format(epoch))\n",
    "        print('train_loss:{:.5f}  acc1:{:.4f}  acc5:{:.4f}  Macro-P:{:.4f}  Macro-R:{:.4f}  Macro-F1:{:.4f}'.format(np.mean(train_loss_list), np.mean(acc1_list), np.mean(acc5_list), precision_score(y_true_list, y_predict_list, average='macro'), recall_score(y_true_list, y_predict_list, average='macro'), f1_score( y_true_list, y_predict_list, average='macro')))\n",
    "        \n",
    "        model.eval()\n",
    "        vaild_loss_list, y_predict_list, y_true_list, acc1_list, acc5_list = [], [], [], [], []\n",
    "        vaild_dataloader = DataLoader(dataset=test_dataset, batch_size=16, sampler=valid_sampler, drop_last=False, collate_fn=collate_fn)\n",
    "        for idx, (seq_x, seq_g, y_true, input_length) in enumerate(vaild_dataloader):\n",
    "            # seq_x, seq_g, y_true = seq_x.cuda(), seq_g.cuda(), y_true.cuda()\n",
    "            seq_g, y_true = seq_g.cuda(), y_true.cuda()\n",
    "            with torch.no_grad():\n",
    "                y_predict = model(seq_x, seq_g, input_length)\n",
    "\n",
    "                y_predict_list.extend(torch.max(y_predict, 1)[1].cpu().numpy().tolist())\n",
    "                y_true_list.extend(y_true.cpu().numpy().tolist())\n",
    "                acc1_list.append(accuracy_1(y_predict, y_true))\n",
    "                acc5_list.append(accuracy_5(y_predict, y_true))\n",
    "\n",
    "                loss = F.nll_loss(y_predict, y_true)\n",
    "                vaild_loss_list.append(loss.item())\n",
    "\n",
    "        print('vaild_loss:{:.5f}  acc1:{:.4f}  acc5:{:.4f}  Macro-P:{:.4f}  Macro-R:{:.4f}  Macro-F1:{:.4f}'.format(np.mean(vaild_loss_list), np.mean(acc1_list), np.mean(acc5_list), precision_score(y_true_list, y_predict_list, average='macro'), recall_score(y_true_list, y_predict_list, average='macro'), f1_score( y_true_list, y_predict_list, average='macro')))\n",
    "\n",
    "        avg_train_losses.append(np.mean(train_loss_list))\n",
    "        avg_valid_losses.append(np.mean(vaild_loss_list))\n",
    "\n",
    "        early_stopping(np.mean(vaild_loss_list), model)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print('Early Stop!')\n",
    "            break\n",
    "\n",
    "        model.load_state_dict(torch.load('../temp/checkpoint.pt'))\n",
    "\n",
    "    return  model, avg_train_losses, avg_valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "patience = 10\n",
    "n_epochs = 60\n",
    "times = 10\n",
    "sum_acc1, sum_acc5, sum_p, sum_r, sum_f1 = 0, 0, 0, 0, 0\n",
    "\n",
    "for idx, seed in enumerate(random.sample(range(0, 1000), times)):\n",
    "\n",
    "    # seed = 555\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    model = T3S().cuda()\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=5e-3)\n",
    "    model, train_loss, valid_loss = train_model(model, patience, n_epochs)\n",
    "\n",
    "    model.eval()\n",
    "    test_dataloader = DataLoader(dataset=test_dataset, batch_size=16, sampler=test_sampler, drop_last=False, collate_fn=collate_fn)\n",
    "    loss_list, acc1_list, acc5_list, y_predict_list, y_true_list = [], [], [], [], []\n",
    "    for idx, (seq_x, seq_g, y_true, input_length) in enumerate(test_dataloader):\n",
    "        # seq_x, seq_g, y_true = seq_x.cuda(), seq_g.cuda(), y_true.cuda()\n",
    "        seq_g, y_true = seq_g.cuda(), y_true.cuda()\n",
    "        with torch.no_grad():\n",
    "            y_predict = model(seq_x, seq_g, input_length)\n",
    "            y_predict_list.extend(torch.max(y_predict, 1)[1].cpu().numpy().tolist())\n",
    "            y_true_list.extend(y_true.cpu().numpy().tolist())\n",
    "            loss = F.nll_loss(y_predict, y_true)\n",
    "            loss_list.append(loss.item())\n",
    "            acc1_list.append(accuracy_1(y_predict, y_true))\n",
    "            acc5_list.append(accuracy_5(y_predict, y_true))\n",
    "    p = precision_score(y_true_list, y_predict_list, average='macro')\n",
    "    r = recall_score(y_true_list, y_predict_list, average='macro')\n",
    "    f1 = f1_score( y_true_list, y_predict_list, average='macro')\n",
    "    sum_acc1 += np.mean(acc1_list)\n",
    "    sum_acc5 += np.mean(acc5_list)\n",
    "    sum_p += p\n",
    "    sum_r += r\n",
    "    sum_f1 += f1\n",
    "    print(\"Testset result:\\n loss_test:{:.5f} \\t acc1_test:{:.4f} \\t  acc5_test:{:.4f} \\t Macro-P:{:.4f} \\t Macro-R:{:.4f} \\t Macro-F1:{:.4f}\".format(np.mean(loss_list), np.mean(acc1_list), np.mean(acc5_list), p, r ,f1))\n",
    "\n",
    "print('acc1:{:.4f}, acc5:{:.4f}, p:{:.4f}, r:{:.4f}, f1:{:.4f}'.format(sum_acc1/5, sum_acc5/5, sum_p/5, sum_r/5, sum_f1/5))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3d6fe1219e01d7dafee20731a05552605019c0a7a7b347a5872aa19530480494"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('py37': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
