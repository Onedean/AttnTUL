{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import math\n",
    "from math import radians, cos, sin, asin, atan2, sqrt, degrees\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备全部数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(lon1, lat1, lon2, lat2):\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    c = 2 * asin(sqrt(sin((lat2 - lat1)/2)**2 + cos(lat1) * cos(lat2) * sin((lon2 - lon1)/2)**2))  # haversine公式\n",
    "    r = 6371.393\n",
    "    return c * r * 1000\n",
    "\n",
    "\n",
    "def conut_gird_num(tracks_data, grid_distance):\n",
    "    Lon1 = tracks_data['Lon'].min()\n",
    "    Lat1 = tracks_data['Lat'].min()\n",
    "    Lon2 = tracks_data['Lon'].max()\n",
    "    Lat2 = tracks_data['Lat'].max()\n",
    "    low = haversine_distance(Lon1,Lat1,Lon2,Lat1)\n",
    "    high = haversine_distance(Lon1,Lat2,Lon2,Lat2)\n",
    "    left = haversine_distance(Lon1,Lat1,Lon1,Lat2)\n",
    "    right = haversine_distance(Lon2,Lat1,Lon2,Lat2)\n",
    "    lon_grid_num = int((low + high) / 2 / grid_distance)\n",
    "    lat_grid_num = int((left + right) / 2 /grid_distance)\n",
    "    print(\"before total:\", lon_grid_num, '*', lat_grid_num, '=', lon_grid_num * lat_grid_num, 'grid')\n",
    "    return lon_grid_num, lat_grid_num\n",
    "\n",
    "\n",
    "def grid_process(tracks_data, grid_distance):\n",
    "    lon_grid_num, lat_grid_num = conut_gird_num(tracks_data, grid_distance)\n",
    "    Lon1 = tracks_data['Lon'].min()\n",
    "    Lon2 = tracks_data['Lon'].max()\n",
    "    Lat1 = tracks_data['Lat'].min()\n",
    "    Lat2 = tracks_data['Lat'].max()\n",
    "    Lon_gap = (Lon2 - Lon1)/lon_grid_num\n",
    "    Lat_gap = (Lat2 - Lat1)/lat_grid_num\n",
    "    tracks_data['grid_ID'] = tracks_data.apply(lambda x: int((x['Lat']-Lat1)/Lat_gap) * lon_grid_num + int((x['Lon']-Lon1)/Lon_gap) + 1, axis=1)\n",
    "    sort_grid = sorted(set(tracks_data['grid_ID']))\n",
    "    tracks_data['grid_ID'] = [sort_grid.index(num) for num in tqdm(tracks_data['grid_ID'])]\n",
    "    print('after total:', len(sort_grid), 'grid')\n",
    "    return tracks_data\n",
    "\n",
    "raw_path = '..\\data\\shenzhen\\shenzhen-all.csv'\n",
    "grid_distance = 111\n",
    "\n",
    "tracks_data = pd.read_csv(raw_path, sep='\\t')\n",
    "tracks_data = grid_process(tracks_data, grid_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_gps = tracks_data.apply(lambda x: str(x['Lon'])+'_'+str(x['Lat']), axis=1).drop_duplicates().values.tolist()\n",
    "gps2idx = {gps:idx+1 for idx, gps in enumerate(all_gps)}\n",
    "gps2idx['pad'] = 0\n",
    "\n",
    "\n",
    "all_grid = tracks_data['grid_ID'].drop_duplicates().values.tolist()\n",
    "grid2idx = {grid:idx+1 for idx, grid in enumerate(all_grid)}\n",
    "grid2idx['pad'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test = 0\n",
    "split_ratio  = 0.6\n",
    "valid_size = 0.5\n",
    "\n",
    "\n",
    "user_list = tracks_data['ObjectID'].drop_duplicates().values.tolist()\n",
    "user_traj_dict = {key:[] for key in user_list}\n",
    "for user_id in tqdm(tracks_data['ObjectID'].drop_duplicates().values.tolist()):\n",
    "    one_user_data = tracks_data.loc[tracks_data.ObjectID == user_id, :]\n",
    "    for traj_id in one_user_data['TrajNumber'].drop_duplicates().values.tolist():\n",
    "        single_grid_data = [grid2idx[grid] for grid in one_user_data.loc[tracks_data.TrajNumber == traj_id, 'grid_ID'].values.tolist()]\n",
    "        single_traj_data = [gps2idx[str(x)+'_'+str(y)] for x,y in zip(one_user_data.loc[tracks_data.TrajNumber == traj_id, 'Lon'].values.tolist(), one_user_data.loc[tracks_data.TrajNumber == traj_id, 'Lat'].values.tolist())]\n",
    "        user_traj_dict[user_id].append((single_grid_data, single_traj_data))\n",
    "\n",
    "user_traj_train, user_traj_test = {key:[] for key in user_list}, {key:[] for key in user_list}\n",
    "\n",
    "for key in user_traj_dict:\n",
    "    traj_num = len(user_traj_dict[key])\n",
    "    num_test += traj_num - int(traj_num * split_ratio)\n",
    "    \n",
    "    for idx in list(range(traj_num))[:int(traj_num * split_ratio)]:\n",
    "        user_traj_train[key].append(user_traj_dict[key][idx])\n",
    "    \n",
    "    for idx in list(range(traj_num))[int(traj_num * split_ratio):]:\n",
    "        user_traj_test[key].append(user_traj_dict[key][idx])\n",
    "\n",
    "\n",
    "print('test::', num_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self, train):\n",
    "        alldata = user_traj_train if train else user_traj_test\n",
    "        self.x_seq, self.g_seq, self.label = [], [], []\n",
    "        for key in alldata:\n",
    "            for one_data in alldata[key]:\n",
    "                single_grid_data, single_traj_data = one_data\n",
    "                self.x_seq.append(single_traj_data)\n",
    "                self.g_seq.append(single_grid_data)\n",
    "                self.label.append(key)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_seq[index], self.g_seq[index], self.label[index], len(self.x_seq[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_seq)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = sorted(batch, key=lambda x: x[-1], reverse=True)\n",
    "    x_contents, g_contents, labels, input_lengths = zip(*batch)\n",
    "    max_len = max([len(content) for content in g_contents])\n",
    "    x_contents = torch.LongTensor([content + [0] * (max_len - len(content)) if len(content) < max_len else content for content in x_contents])\n",
    "    g_contents = torch.LongTensor([content + [0] * (max_len - len(content)) if len(content) < max_len else content for content in g_contents])\n",
    "    labels = torch.LongTensor(labels)\n",
    "    input_lengths = torch.LongTensor(input_lengths)\n",
    "    return x_contents, g_contents, labels, input_lengths\n",
    "\n",
    "\n",
    "indices = list(range(num_test))\n",
    "np.random.seed(555)\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(num_test * valid_size))\n",
    "valid_idx, test_idx = indices[split:], indices[:split]\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "test_sampler = SubsetRandomSampler(test_idx)\n",
    "\n",
    "\n",
    "train_dataset = myDataset(train=True)\n",
    "test_dataset = myDataset(train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    }
   ],
   "source": [
    "n_class = len(user_traj_dict)\n",
    "n_hidden = 128\n",
    "grid_embedding_dim = 128\n",
    "print(n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence,pad_packed_sequence\n",
    "\n",
    "class T3S(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(T3S, self).__init__()\n",
    "        # self.emb1 = nn.Embedding(len(gps2idx), 250, padding_idx=0)\n",
    "        self.emb1 = nn.Embedding(len(grid2idx), 250, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=250, hidden_size=n_hidden, batch_first=True, bidirectional=False)\n",
    "        self.emb2 = nn.Embedding(len(grid2idx), 128, padding_idx=0)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=128, nhead=16)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        self.fc = nn.Linear(n_hidden, n_class)\n",
    "\n",
    "    def forward(self, seq_x, seq_g, input_length):\n",
    "        # seq_x = self.emb1(seq_x)\n",
    "        input = self.emb1(seq_g)\n",
    "        x_packed = pack_padded_sequence(input, input_length, batch_first=True)\n",
    "        o_n, (h_n, _) = self.lstm(x_packed)\n",
    "        lstm_output = h_n[-1, :, :]\n",
    "        seq_g = self.emb2(seq_g)\n",
    "        attn_output = self.transformer_encoder(seq_g).mean(dim=1)\n",
    "        \n",
    "        output = self.fc((lstm_output + attn_output))\n",
    "        # output = self.fc(attn_output)\n",
    "        return F.log_softmax(output, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_1(pred, targ):\n",
    "    pred = torch.max(pred, 1)[1]\n",
    "    ac = ((pred == targ).float()).sum().item() / targ.size()[0]\n",
    "    return ac\n",
    "\n",
    "def accuracy_5(pred,targ):\n",
    "    pred = torch.topk(pred, k=5, dim=1, largest=True, sorted=True)[1]\n",
    "    ac = (torch.tensor([t in p for p,t in zip(pred,targ)]).float()).sum().item() / targ.size()[0]\n",
    "    return ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "        \n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), '../temp/checkpoint.pt')\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def train_model(model, patience, n_epochs):\n",
    "    avg_train_losses = []\n",
    "    avg_valid_losses = []\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss_list, y_predict_list, y_true_list, acc1_list, acc5_list = [], [], [], [], []\n",
    "        train_dataloader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True, drop_last=False, collate_fn=collate_fn)\n",
    "        for idx, (seq_x, seq_g, y_true, input_length) in enumerate(train_dataloader):\n",
    "            # seq_x, seq_g, y_true = seq_x.cuda(), seq_g.cuda(), y_true.cuda()\n",
    "            seq_g, y_true = seq_g.cuda(), y_true.cuda()\n",
    "            y_predict = model(seq_x, seq_g, input_length)\n",
    "            y_predict_list.extend(torch.max(y_predict, 1)[1].cpu().numpy().tolist())\n",
    "            y_true_list.extend(y_true.cpu().numpy().tolist())\n",
    "            acc1_list.append(accuracy_1(y_predict, y_true))\n",
    "            acc5_list.append(accuracy_5(y_predict, y_true))\n",
    "            \n",
    "            loss = F.nll_loss(y_predict, y_true)\n",
    "            train_loss_list.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print('Epoch: {}'.format(epoch))\n",
    "        print('train_loss:{:.5f}  acc1:{:.4f}  acc5:{:.4f}  Macro-P:{:.4f}  Macro-R:{:.4f}  Macro-F1:{:.4f}'.format(np.mean(train_loss_list), np.mean(acc1_list), np.mean(acc5_list), precision_score(y_true_list, y_predict_list, average='macro'), recall_score(y_true_list, y_predict_list, average='macro'), f1_score( y_true_list, y_predict_list, average='macro')))\n",
    "        \n",
    "        model.eval()\n",
    "        vaild_loss_list, y_predict_list, y_true_list, acc1_list, acc5_list = [], [], [], [], []\n",
    "        vaild_dataloader = DataLoader(dataset=test_dataset, batch_size=16, sampler=valid_sampler, drop_last=False, collate_fn=collate_fn)\n",
    "        for idx, (seq_x, seq_g, y_true, input_length) in enumerate(vaild_dataloader):\n",
    "            # seq_x, seq_g, y_true = seq_x.cuda(), seq_g.cuda(), y_true.cuda()\n",
    "            seq_g, y_true = seq_g.cuda(), y_true.cuda()\n",
    "            with torch.no_grad():\n",
    "                y_predict = model(seq_x, seq_g, input_length)\n",
    "\n",
    "                y_predict_list.extend(torch.max(y_predict, 1)[1].cpu().numpy().tolist())\n",
    "                y_true_list.extend(y_true.cpu().numpy().tolist())\n",
    "                acc1_list.append(accuracy_1(y_predict, y_true))\n",
    "                acc5_list.append(accuracy_5(y_predict, y_true))\n",
    "\n",
    "                loss = F.nll_loss(y_predict, y_true)\n",
    "                vaild_loss_list.append(loss.item())\n",
    "\n",
    "        print('vaild_loss:{:.5f}  acc1:{:.4f}  acc5:{:.4f}  Macro-P:{:.4f}  Macro-R:{:.4f}  Macro-F1:{:.4f}'.format(np.mean(vaild_loss_list), np.mean(acc1_list), np.mean(acc5_list), precision_score(y_true_list, y_predict_list, average='macro'), recall_score(y_true_list, y_predict_list, average='macro'), f1_score( y_true_list, y_predict_list, average='macro')))\n",
    "\n",
    "        avg_train_losses.append(np.mean(train_loss_list))\n",
    "        avg_valid_losses.append(np.mean(vaild_loss_list))\n",
    "\n",
    "        early_stopping(np.mean(vaild_loss_list), model)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print('Early Stop!')\n",
    "            break\n",
    "\n",
    "        model.load_state_dict(torch.load('../temp/checkpoint.pt'))\n",
    "\n",
    "    return  model, avg_train_losses, avg_valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "train_loss:4.12171  acc1:0.0887  acc5:0.2233  Macro-P:0.0986  Macro-R:0.0610  Macro-F1:0.0607\n",
      "vaild_loss:3.77480  acc1:0.1379  acc5:0.3036  Macro-P:0.1778  Macro-R:0.0967  Macro-F1:0.0999\n",
      "Validation loss decreased (inf --> 3.774803).  Saving model ...\n",
      "Epoch: 1\n",
      "train_loss:2.73981  acc1:0.3448  acc5:0.6376  Macro-P:0.3535  Macro-R:0.3064  Macro-F1:0.3038\n",
      "vaild_loss:3.61120  acc1:0.2027  acc5:0.3755  Macro-P:0.2241  Macro-R:0.1953  Macro-F1:0.1841\n",
      "Validation loss decreased (3.774803 --> 3.611202).  Saving model ...\n",
      "Epoch: 2\n",
      "train_loss:1.66432  acc1:0.5948  acc5:0.8614  Macro-P:0.6065  Macro-R:0.5623  Macro-F1:0.5706\n",
      "vaild_loss:3.89414  acc1:0.1982  acc5:0.3688  Macro-P:0.2296  Macro-R:0.2058  Macro-F1:0.1934\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch: 3\n",
      "train_loss:1.57863  acc1:0.6018  acc5:0.8604  Macro-P:0.5984  Macro-R:0.5783  Macro-F1:0.5791\n",
      "vaild_loss:3.88597  acc1:0.2053  acc5:0.3710  Macro-P:0.2492  Macro-R:0.1957  Macro-F1:0.1941\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch: 4\n",
      "train_loss:1.57292  acc1:0.6149  acc5:0.8705  Macro-P:0.6220  Macro-R:0.5886  Macro-F1:0.5945\n",
      "vaild_loss:3.87500  acc1:0.2215  acc5:0.3738  Macro-P:0.2319  Macro-R:0.2093  Macro-F1:0.2040\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch: 5\n",
      "train_loss:1.56408  acc1:0.6094  acc5:0.8700  Macro-P:0.6174  Macro-R:0.5839  Macro-F1:0.5892\n",
      "vaild_loss:3.86782  acc1:0.2143  acc5:0.3610  Macro-P:0.2085  Macro-R:0.2117  Macro-F1:0.1972\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch: 6\n",
      "train_loss:1.54352  acc1:0.6205  acc5:0.8755  Macro-P:0.6300  Macro-R:0.5941  Macro-F1:0.6005\n",
      "vaild_loss:3.89439  acc1:0.2258  acc5:0.3660  Macro-P:0.2274  Macro-R:0.2157  Macro-F1:0.2051\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Epoch: 7\n",
      "train_loss:1.57287  acc1:0.6069  acc5:0.8634  Macro-P:0.6112  Macro-R:0.5854  Macro-F1:0.5887\n",
      "vaild_loss:3.84507  acc1:0.2041  acc5:0.3733  Macro-P:0.2186  Macro-R:0.2011  Macro-F1:0.1971\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Epoch: 8\n",
      "train_loss:1.54775  acc1:0.6089  acc5:0.8750  Macro-P:0.6189  Macro-R:0.5818  Macro-F1:0.5915\n",
      "vaild_loss:3.93810  acc1:0.2149  acc5:0.3681  Macro-P:0.2434  Macro-R:0.2149  Macro-F1:0.2076\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Epoch: 9\n",
      "train_loss:1.55357  acc1:0.6144  acc5:0.8695  Macro-P:0.6273  Macro-R:0.5967  Macro-F1:0.6014\n",
      "vaild_loss:3.98648  acc1:0.2113  acc5:0.3544  Macro-P:0.2262  Macro-R:0.1953  Macro-F1:0.1933\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Epoch: 10\n",
      "train_loss:1.55052  acc1:0.6139  acc5:0.8700  Macro-P:0.6203  Macro-R:0.5975  Macro-F1:0.5987\n",
      "vaild_loss:3.94813  acc1:0.2027  acc5:0.3451  Macro-P:0.2631  Macro-R:0.1975  Macro-F1:0.2000\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Epoch: 11\n",
      "train_loss:1.55075  acc1:0.6099  acc5:0.8730  Macro-P:0.6203  Macro-R:0.5917  Macro-F1:0.5951\n",
      "vaild_loss:3.95370  acc1:0.2034  acc5:0.3696  Macro-P:0.2156  Macro-R:0.1912  Macro-F1:0.1915\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early Stop!\n",
      "Testset result:\n",
      " loss_test:3.83583 \t acc1_test:0.2140 \t  acc5_test:0.3689 \t Macro-P:0.2754 \t Macro-R:0.2020 \t Macro-F1:0.2089\n",
      "Epoch: 0\n",
      "train_loss:4.10977  acc1:0.0902  acc5:0.2167  Macro-P:0.1085  Macro-R:0.0640  Macro-F1:0.0642\n",
      "vaild_loss:3.87017  acc1:0.1342  acc5:0.2787  Macro-P:0.1309  Macro-R:0.0869  Macro-F1:0.0715\n",
      "Validation loss decreased (inf --> 3.870169).  Saving model ...\n",
      "Epoch: 1\n",
      "train_loss:2.80658  acc1:0.3306  acc5:0.6079  Macro-P:0.3483  Macro-R:0.2941  Macro-F1:0.2850\n",
      "vaild_loss:3.60029  acc1:0.2077  acc5:0.3691  Macro-P:0.2713  Macro-R:0.1939  Macro-F1:0.1925\n",
      "Validation loss decreased (3.870169 --> 3.600287).  Saving model ...\n",
      "Epoch: 2\n",
      "train_loss:1.73687  acc1:0.5660  acc5:0.8558  Macro-P:0.5839  Macro-R:0.5333  Macro-F1:0.5394\n",
      "vaild_loss:3.70278  acc1:0.2164  acc5:0.3783  Macro-P:0.2464  Macro-R:0.2025  Macro-F1:0.2098\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch: 3\n",
      "train_loss:1.67561  acc1:0.5781  acc5:0.8649  Macro-P:0.5859  Macro-R:0.5552  Macro-F1:0.5563\n",
      "vaild_loss:3.69929  acc1:0.2171  acc5:0.4008  Macro-P:0.2342  Macro-R:0.2039  Macro-F1:0.2027\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch: 4\n",
      "train_loss:1.66456  acc1:0.5988  acc5:0.8538  Macro-P:0.6144  Macro-R:0.5680  Macro-F1:0.5755\n",
      "vaild_loss:3.75231  acc1:0.2127  acc5:0.3857  Macro-P:0.2314  Macro-R:0.1902  Macro-F1:0.1902\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch: 5\n",
      "train_loss:1.64449  acc1:0.5983  acc5:0.8720  Macro-P:0.6075  Macro-R:0.5678  Macro-F1:0.5750\n",
      "vaild_loss:3.75843  acc1:0.2150  acc5:0.3697  Macro-P:0.2350  Macro-R:0.1936  Macro-F1:0.2017\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch: 6\n",
      "train_loss:1.64147  acc1:0.5993  acc5:0.8669  Macro-P:0.6109  Macro-R:0.5700  Macro-F1:0.5788\n",
      "vaild_loss:3.74184  acc1:0.2208  acc5:0.3747  Macro-P:0.2477  Macro-R:0.2061  Macro-F1:0.2100\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Epoch: 7\n",
      "train_loss:1.62105  acc1:0.6069  acc5:0.8649  Macro-P:0.6232  Macro-R:0.5835  Macro-F1:0.5902\n",
      "vaild_loss:3.75079  acc1:0.2274  acc5:0.3857  Macro-P:0.2409  Macro-R:0.2195  Macro-F1:0.2056\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Epoch: 8\n",
      "train_loss:1.61986  acc1:0.6094  acc5:0.8690  Macro-P:0.6105  Macro-R:0.5784  Macro-F1:0.5837\n",
      "vaild_loss:3.72867  acc1:0.2289  acc5:0.3749  Macro-P:0.2687  Macro-R:0.2168  Macro-F1:0.2174\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Epoch: 9\n",
      "train_loss:1.63628  acc1:0.5912  acc5:0.8690  Macro-P:0.5962  Macro-R:0.5630  Macro-F1:0.5683\n",
      "vaild_loss:3.77006  acc1:0.2216  acc5:0.3763  Macro-P:0.2500  Macro-R:0.2117  Macro-F1:0.2137\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Epoch: 10\n",
      "train_loss:1.62329  acc1:0.6058  acc5:0.8649  Macro-P:0.6204  Macro-R:0.5733  Macro-F1:0.5834\n",
      "vaild_loss:3.73712  acc1:0.2228  acc5:0.3870  Macro-P:0.2674  Macro-R:0.1996  Macro-F1:0.2098\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Epoch: 11\n",
      "train_loss:1.62238  acc1:0.6064  acc5:0.8690  Macro-P:0.6107  Macro-R:0.5716  Macro-F1:0.5745\n",
      "vaild_loss:3.75465  acc1:0.2164  acc5:0.3705  Macro-P:0.2553  Macro-R:0.2038  Macro-F1:0.1993\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early Stop!\n",
      "Testset result:\n",
      " loss_test:3.66618 \t acc1_test:0.2174 \t  acc5_test:0.4177 \t Macro-P:0.2807 \t Macro-R:0.2002 \t Macro-F1:0.2136\n",
      "Epoch: 0\n",
      "train_loss:4.14632  acc1:0.0852  acc5:0.2213  Macro-P:0.0991  Macro-R:0.0574  Macro-F1:0.0585\n",
      "vaild_loss:3.75988  acc1:0.1518  acc5:0.3188  Macro-P:0.1526  Macro-R:0.1140  Macro-F1:0.1022\n",
      "Validation loss decreased (inf --> 3.759884).  Saving model ...\n",
      "Epoch: 1\n",
      "train_loss:2.85905  acc1:0.3196  acc5:0.6048  Macro-P:0.3139  Macro-R:0.2742  Macro-F1:0.2719\n",
      "vaild_loss:3.58754  acc1:0.1838  acc5:0.3798  Macro-P:0.2249  Macro-R:0.1572  Macro-F1:0.1634\n",
      "Validation loss decreased (3.759884 --> 3.587539).  Saving model ...\n",
      "Epoch: 2\n",
      "train_loss:1.76589  acc1:0.5685  acc5:0.8296  Macro-P:0.5647  Macro-R:0.5420  Macro-F1:0.5431\n",
      "vaild_loss:3.72192  acc1:0.2026  acc5:0.3848  Macro-P:0.2264  Macro-R:0.1942  Macro-F1:0.1916\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch: 3\n",
      "train_loss:1.70719  acc1:0.5685  acc5:0.8543  Macro-P:0.5628  Macro-R:0.5346  Macro-F1:0.5369\n",
      "vaild_loss:3.78835  acc1:0.1982  acc5:0.3842  Macro-P:0.2146  Macro-R:0.1944  Macro-F1:0.1914\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch: 4\n",
      "train_loss:1.69500  acc1:0.5786  acc5:0.8614  Macro-P:0.5804  Macro-R:0.5536  Macro-F1:0.5531\n",
      "vaild_loss:3.80683  acc1:0.2171  acc5:0.3849  Macro-P:0.2658  Macro-R:0.1998  Macro-F1:0.2056\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch: 5\n",
      "train_loss:1.69241  acc1:0.5801  acc5:0.8427  Macro-P:0.5767  Macro-R:0.5543  Macro-F1:0.5529\n",
      "vaild_loss:3.75826  acc1:0.2119  acc5:0.3798  Macro-P:0.2394  Macro-R:0.1987  Macro-F1:0.1961\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch: 6\n",
      "train_loss:1.70791  acc1:0.5796  acc5:0.8523  Macro-P:0.5915  Macro-R:0.5593  Macro-F1:0.5600\n",
      "vaild_loss:3.72753  acc1:0.2179  acc5:0.3820  Macro-P:0.2177  Macro-R:0.2002  Macro-F1:0.1914\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Epoch: 7\n",
      "train_loss:1.69392  acc1:0.5817  acc5:0.8579  Macro-P:0.5818  Macro-R:0.5526  Macro-F1:0.5552\n",
      "vaild_loss:3.74469  acc1:0.2027  acc5:0.3973  Macro-P:0.2453  Macro-R:0.1964  Macro-F1:0.2025\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Epoch: 8\n",
      "train_loss:1.68682  acc1:0.5968  acc5:0.8629  Macro-P:0.6113  Macro-R:0.5696  Macro-F1:0.5728\n",
      "vaild_loss:3.79635  acc1:0.1954  acc5:0.4058  Macro-P:0.2366  Macro-R:0.1791  Macro-F1:0.1872\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Epoch: 9\n",
      "train_loss:1.70143  acc1:0.5811  acc5:0.8468  Macro-P:0.5901  Macro-R:0.5547  Macro-F1:0.5605\n",
      "vaild_loss:3.80748  acc1:0.2184  acc5:0.3695  Macro-P:0.2613  Macro-R:0.2134  Macro-F1:0.2122\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Epoch: 10\n",
      "train_loss:1.68956  acc1:0.5953  acc5:0.8518  Macro-P:0.6049  Macro-R:0.5691  Macro-F1:0.5718\n",
      "vaild_loss:3.82409  acc1:0.1831  acc5:0.3742  Macro-P:0.2541  Macro-R:0.1717  Macro-F1:0.1820\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Epoch: 11\n",
      "train_loss:1.69402  acc1:0.5817  acc5:0.8589  Macro-P:0.5764  Macro-R:0.5526  Macro-F1:0.5531\n",
      "vaild_loss:3.77073  acc1:0.2114  acc5:0.3931  Macro-P:0.2527  Macro-R:0.2043  Macro-F1:0.2110\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early Stop!\n",
      "Testset result:\n",
      " loss_test:3.64144 \t acc1_test:0.2160 \t  acc5_test:0.4032 \t Macro-P:0.2680 \t Macro-R:0.2078 \t Macro-F1:0.2131\n",
      "Epoch: 0\n",
      "train_loss:4.12368  acc1:0.0857  acc5:0.2102  Macro-P:0.0835  Macro-R:0.0578  Macro-F1:0.0555\n",
      "vaild_loss:3.71323  acc1:0.1533  acc5:0.3051  Macro-P:0.1660  Macro-R:0.1176  Macro-F1:0.1067\n",
      "Validation loss decreased (inf --> 3.713229).  Saving model ...\n",
      "Epoch: 1\n",
      "train_loss:2.78754  acc1:0.3357  acc5:0.6255  Macro-P:0.3455  Macro-R:0.3002  Macro-F1:0.2973\n",
      "vaild_loss:3.66311  acc1:0.1822  acc5:0.3281  Macro-P:0.1810  Macro-R:0.1637  Macro-F1:0.1478\n",
      "Validation loss decreased (3.713229 --> 3.663107).  Saving model ...\n",
      "Epoch: 2\n",
      "train_loss:1.71482  acc1:0.5751  acc5:0.8639  Macro-P:0.5849  Macro-R:0.5497  Macro-F1:0.5548\n",
      "vaild_loss:3.82738  acc1:0.2035  acc5:0.3755  Macro-P:0.2531  Macro-R:0.1914  Macro-F1:0.1964\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch: 3\n",
      "train_loss:1.67103  acc1:0.5917  acc5:0.8584  Macro-P:0.6053  Macro-R:0.5725  Macro-F1:0.5771\n",
      "vaild_loss:3.89031  acc1:0.1974  acc5:0.3521  Macro-P:0.2474  Macro-R:0.1837  Macro-F1:0.1937\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch: 4\n",
      "train_loss:1.64502  acc1:0.5958  acc5:0.8609  Macro-P:0.5963  Macro-R:0.5721  Macro-F1:0.5719\n",
      "vaild_loss:3.87307  acc1:0.2020  acc5:0.3576  Macro-P:0.2698  Macro-R:0.1909  Macro-F1:0.2039\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch: 5\n",
      "train_loss:1.64177  acc1:0.6013  acc5:0.8664  Macro-P:0.6032  Macro-R:0.5746  Macro-F1:0.5774\n",
      "vaild_loss:3.86274  acc1:0.1982  acc5:0.3565  Macro-P:0.2604  Macro-R:0.1883  Macro-F1:0.1947\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch: 6\n",
      "train_loss:1.64912  acc1:0.5932  acc5:0.8574  Macro-P:0.6098  Macro-R:0.5730  Macro-F1:0.5780\n",
      "vaild_loss:3.86074  acc1:0.1946  acc5:0.3654  Macro-P:0.2379  Macro-R:0.1892  Macro-F1:0.1921\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Epoch: 7\n",
      "train_loss:1.62831  acc1:0.6033  acc5:0.8710  Macro-P:0.6118  Macro-R:0.5828  Macro-F1:0.5880\n",
      "vaild_loss:3.85401  acc1:0.1911  acc5:0.3713  Macro-P:0.2325  Macro-R:0.1808  Macro-F1:0.1768\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Epoch: 8\n",
      "train_loss:1.63623  acc1:0.6043  acc5:0.8649  Macro-P:0.6203  Macro-R:0.5851  Macro-F1:0.5907\n",
      "vaild_loss:3.84275  acc1:0.2004  acc5:0.3623  Macro-P:0.2226  Macro-R:0.1934  Macro-F1:0.1871\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Epoch: 9\n",
      "train_loss:1.63400  acc1:0.5983  acc5:0.8654  Macro-P:0.6069  Macro-R:0.5742  Macro-F1:0.5803\n",
      "vaild_loss:3.84321  acc1:0.2068  acc5:0.3564  Macro-P:0.2139  Macro-R:0.1915  Macro-F1:0.1855\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Epoch: 10\n",
      "train_loss:1.62514  acc1:0.6079  acc5:0.8604  Macro-P:0.6166  Macro-R:0.5810  Macro-F1:0.5882\n",
      "vaild_loss:3.84935  acc1:0.1911  acc5:0.3610  Macro-P:0.2099  Macro-R:0.1856  Macro-F1:0.1799\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Epoch: 11\n",
      "train_loss:1.61981  acc1:0.5993  acc5:0.8679  Macro-P:0.6173  Macro-R:0.5770  Macro-F1:0.5815\n",
      "vaild_loss:3.92039  acc1:0.1846  acc5:0.3698  Macro-P:0.2058  Macro-R:0.1767  Macro-F1:0.1722\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early Stop!\n",
      "Testset result:\n",
      " loss_test:3.87976 \t acc1_test:0.2093 \t  acc5_test:0.3558 \t Macro-P:0.2890 \t Macro-R:0.1926 \t Macro-F1:0.2020\n",
      "Epoch: 0\n",
      "train_loss:4.13110  acc1:0.0822  acc5:0.2107  Macro-P:0.1291  Macro-R:0.0597  Macro-F1:0.0641\n",
      "vaild_loss:3.77416  acc1:0.1633  acc5:0.2999  Macro-P:0.2023  Macro-R:0.1371  Macro-F1:0.1290\n",
      "Validation loss decreased (inf --> 3.774159).  Saving model ...\n",
      "Epoch: 1\n",
      "train_loss:2.76237  acc1:0.3417  acc5:0.6240  Macro-P:0.3365  Macro-R:0.3094  Macro-F1:0.3038\n",
      "vaild_loss:3.69526  acc1:0.1954  acc5:0.3502  Macro-P:0.2491  Macro-R:0.1803  Macro-F1:0.1860\n",
      "Validation loss decreased (3.774159 --> 3.695259).  Saving model ...\n",
      "Epoch: 2\n",
      "train_loss:1.71945  acc1:0.5771  acc5:0.8468  Macro-P:0.6021  Macro-R:0.5549  Macro-F1:0.5616\n",
      "vaild_loss:3.81393  acc1:0.2179  acc5:0.3487  Macro-P:0.2456  Macro-R:0.2125  Macro-F1:0.2086\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch: 3\n",
      "train_loss:1.68432  acc1:0.5917  acc5:0.8473  Macro-P:0.5859  Macro-R:0.5669  Macro-F1:0.5679\n",
      "vaild_loss:3.79615  acc1:0.2055  acc5:0.3710  Macro-P:0.2072  Macro-R:0.1885  Macro-F1:0.1851\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch: 4\n",
      "train_loss:1.66415  acc1:0.5877  acc5:0.8584  Macro-P:0.5876  Macro-R:0.5577  Macro-F1:0.5564\n",
      "vaild_loss:3.84119  acc1:0.2039  acc5:0.3775  Macro-P:0.2310  Macro-R:0.1924  Macro-F1:0.1932\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch: 5\n",
      "train_loss:1.64490  acc1:0.5892  acc5:0.8664  Macro-P:0.6025  Macro-R:0.5676  Macro-F1:0.5717\n",
      "vaild_loss:3.79637  acc1:0.2071  acc5:0.3835  Macro-P:0.2214  Macro-R:0.1997  Macro-F1:0.1925\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch: 6\n",
      "train_loss:1.63423  acc1:0.5998  acc5:0.8609  Macro-P:0.6139  Macro-R:0.5713  Macro-F1:0.5784\n",
      "vaild_loss:3.82123  acc1:0.2048  acc5:0.3804  Macro-P:0.2162  Macro-R:0.1906  Macro-F1:0.1857\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Epoch: 7\n",
      "train_loss:1.64398  acc1:0.5973  acc5:0.8584  Macro-P:0.6085  Macro-R:0.5798  Macro-F1:0.5842\n",
      "vaild_loss:3.76732  acc1:0.2069  acc5:0.3675  Macro-P:0.2158  Macro-R:0.2019  Macro-F1:0.1945\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Epoch: 8\n",
      "train_loss:1.63487  acc1:0.6089  acc5:0.8649  Macro-P:0.6188  Macro-R:0.5850  Macro-F1:0.5897\n",
      "vaild_loss:3.84008  acc1:0.2018  acc5:0.3761  Macro-P:0.2179  Macro-R:0.1881  Macro-F1:0.1876\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Epoch: 9\n",
      "train_loss:1.62540  acc1:0.5988  acc5:0.8715  Macro-P:0.6031  Macro-R:0.5731  Macro-F1:0.5773\n",
      "vaild_loss:3.82654  acc1:0.2034  acc5:0.3705  Macro-P:0.2454  Macro-R:0.2007  Macro-F1:0.2042\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Epoch: 10\n",
      "train_loss:1.66769  acc1:0.5892  acc5:0.8518  Macro-P:0.5976  Macro-R:0.5712  Macro-F1:0.5733\n",
      "vaild_loss:3.85026  acc1:0.2069  acc5:0.3639  Macro-P:0.2346  Macro-R:0.1875  Macro-F1:0.1902\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Epoch: 11\n",
      "train_loss:1.66195  acc1:0.5877  acc5:0.8574  Macro-P:0.5968  Macro-R:0.5748  Macro-F1:0.5770\n",
      "vaild_loss:3.81148  acc1:0.2119  acc5:0.3805  Macro-P:0.2082  Macro-R:0.1998  Macro-F1:0.1889\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early Stop!\n",
      "Testset result:\n",
      " loss_test:3.73359 \t acc1_test:0.2128 \t  acc5_test:0.3945 \t Macro-P:0.2413 \t Macro-R:0.2005 \t Macro-F1:0.2001\n",
      "acc1:0.2139, acc5:0.3880, p:0.2709, r:0.2006, f1:0.2075\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "patience = 10\n",
    "n_epochs = 60\n",
    "times = 5\n",
    "sum_acc1, sum_acc5, sum_p, sum_r, sum_f1 = 0, 0, 0, 0, 0\n",
    "\n",
    "for idx, seed in enumerate(random.sample(range(0, 1000), times)):\n",
    "\n",
    "    # seed = 555\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    model = T3S().cuda()\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=5e-3)\n",
    "    model, train_loss, valid_loss = train_model(model, patience, n_epochs)\n",
    "\n",
    "    model.eval()\n",
    "    test_dataloader = DataLoader(dataset=test_dataset, batch_size=16, sampler=test_sampler, drop_last=False, collate_fn=collate_fn)\n",
    "    loss_list, acc1_list, acc5_list, y_predict_list, y_true_list = [], [], [], [], []\n",
    "    for idx, (seq_x, seq_g, y_true, input_length) in enumerate(test_dataloader):\n",
    "        # seq_x, seq_g, y_true = seq_x.cuda(), seq_g.cuda(), y_true.cuda()\n",
    "        seq_g, y_true = seq_g.cuda(), y_true.cuda()\n",
    "        with torch.no_grad():\n",
    "            y_predict = model(seq_x, seq_g, input_length)\n",
    "            y_predict_list.extend(torch.max(y_predict, 1)[1].cpu().numpy().tolist())\n",
    "            y_true_list.extend(y_true.cpu().numpy().tolist())\n",
    "            loss = F.nll_loss(y_predict, y_true)\n",
    "            loss_list.append(loss.item())\n",
    "            acc1_list.append(accuracy_1(y_predict, y_true))\n",
    "            acc5_list.append(accuracy_5(y_predict, y_true))\n",
    "    p = precision_score(y_true_list, y_predict_list, average='macro')\n",
    "    r = recall_score(y_true_list, y_predict_list, average='macro')\n",
    "    f1 = f1_score( y_true_list, y_predict_list, average='macro')\n",
    "    sum_acc1 += np.mean(acc1_list)\n",
    "    sum_acc5 += np.mean(acc5_list)\n",
    "    sum_p += p\n",
    "    sum_r += r\n",
    "    sum_f1 += f1\n",
    "    print(\"Testset result:\\n loss_test:{:.5f} \\t acc1_test:{:.4f} \\t  acc5_test:{:.4f} \\t Macro-P:{:.4f} \\t Macro-R:{:.4f} \\t Macro-F1:{:.4f}\".format(np.mean(loss_list), np.mean(acc1_list), np.mean(acc5_list), p, r ,f1))\n",
    "\n",
    "print('acc1:{:.4f}, acc5:{:.4f}, p:{:.4f}, r:{:.4f}, f1:{:.4f}'.format(sum_acc1/5, sum_acc5/5, sum_p/5, sum_r/5, sum_f1/5))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3d6fe1219e01d7dafee20731a05552605019c0a7a7b347a5872aa19530480494"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('py37': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
